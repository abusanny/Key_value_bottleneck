# -*- coding: utf-8 -*-
"""key_value_bottleneck.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zCdFG1BD7suhVs0nNjSShXKEP5M2Lj5U
"""

# Install necessary packages
!pip install timm
!pip install git+https://github.com/openai/CLIP.git

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms, models
import torchvision
import timm  # For SwAV and DINO models
import clip  # For CLIP model
import numpy as np
import random
import os
from tqdm import tqdm

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Set random seed for reproducibility
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

set_seed(42)

def get_resnet50_imagenet():
    model = models.resnet50(pretrained=True)
    feature_dim = model.fc.in_features

    # Remove the final classification layer
    modules = list(model.children())[:-1]
    model = nn.Sequential(*modules)
    model.eval()
    return model.to(device), feature_dim

def get_clip_vit_b32():
    model, _ = clip.load("ViT-B/32", device=device)
    feature_dim = model.visual.output_dim
    model = model.visual  # Use the visual part only
    model.eval()
    return model.to(device), feature_dim

def get_swav_resnet50w2():
    # SwAV ResNet50w2 model
    model = timm.create_model('swav_resnet50w2', pretrained=True)
    feature_dim = model.fc.in_features
    # Remove the final classification layer
    modules = list(model.children())[:-1]
    model = nn.Sequential(*modules)
    model.eval()
    return model.to(device), feature_dim

def get_dino_resnet50():
    # DINO ResNet50 model
    model = timm.create_model('dino_resnet50', pretrained=True)
    feature_dim = model.num_features
    # Remove the final classification layer
    modules = list(model.children())[:-1]
    model = nn.Sequential(*modules)
    model.eval()
    return model.to(device), feature_dim

class KeyValueBottleneck(nn.Module):
    def __init__(self, num_codebooks, codebook_size, key_dim, value_dim, ema_decay=0.95):
        super(KeyValueBottleneck, self).__init__()
        self.num_codebooks = num_codebooks
        self.codebook_size = codebook_size
        self.key_dim = key_dim
        self.value_dim = value_dim
        self.ema_decay = ema_decay

        # Initialize keys and values
        self.keys = nn.Parameter(torch.randn(num_codebooks, codebook_size, key_dim))
        self.values = nn.Parameter(torch.randn(num_codebooks, codebook_size, value_dim))

        # Key usage counts for thresholding
        self.register_buffer('key_usage_counts', torch.zeros(num_codebooks, codebook_size))

        # Flags
        self.initialized = False

    def forward(self, x):
        # x shape: (batch_size, key_dim)
        batch_size = x.size(0)
        num_codebooks = self.num_codebooks
        codebook_size = self.codebook_size

        selected_values = []

        for i in range(num_codebooks):
            keys_i = self.keys[i]  # (codebook_size, key_dim)
            values_i = self.values[i]  # (codebook_size, value_dim)

            # Compute distances between x and keys_i
            # Using torch.cdist for efficient computation
            distances = torch.cdist(x, keys_i, p=2)  # (batch_size, codebook_size)

            # Find nearest key
            indices_i = distances.argmin(dim=1)  # (batch_size)

            # Gather values
            selected_values_i = values_i[indices_i]  # (batch_size, value_dim)
            selected_values.append(selected_values_i)

            if self.training:
                if not self.initialized:
                    # Initialize keys with first batch
                    with torch.no_grad():
                        self.keys.data[i].index_copy_(0, indices_i, x)
                else:
                    # EMA update
                    keys_selected = self.keys[i][indices_i]  # (batch_size, key_dim)
                    new_keys = self.ema_decay * keys_selected + (1 - self.ema_decay) * x

                    # Accumulate updates to handle duplicate indices
                    delta_keys = new_keys - keys_selected  # (batch_size, key_dim)
                    keys_update = torch.zeros_like(keys_i)  # (codebook_size, key_dim)
                    keys_update.index_add_(0, indices_i, delta_keys)

                    # Update keys
                    self.keys.data[i] += keys_update

                    # Update key usage counts
                    usage = torch.bincount(indices_i, minlength=codebook_size).float().to(x.device)
                    self.key_usage_counts[i] = self.ema_decay * self.key_usage_counts[i] + (1 - self.ema_decay) * usage

        selected_values = torch.cat(selected_values, dim=1)  # (batch_size, num_codebooks * value_dim)

        if self.training and not self.initialized:
            self.initialized = True

        return selected_values


    def reinitialize_keys(self, threshold):
        # Re-initialize keys with usage below the threshold
        low_usage_mask = self.key_usage_counts < threshold
        num_reinit = low_usage_mask.sum().item()
        if num_reinit > 0:
            with torch.no_grad():
                for i in range(self.num_codebooks):
                    low_usage_indices = low_usage_mask[i].nonzero(as_tuple=False).squeeze()
                    if low_usage_indices.numel() > 0:
                        self.keys.data[i, low_usage_indices, :] = torch.randn_like(self.keys.data[i, low_usage_indices, :])
                        self.values.data[i, low_usage_indices, :] = torch.randn_like(self.values.data[i, low_usage_indices, :])
                        self.key_usage_counts[i, low_usage_indices] = 0


    def reinitialize_keys(self, threshold):
        # Re-initialize keys with usage below the threshold
        low_usage_mask = self.key_usage_counts < threshold
        num_reinit = low_usage_mask.sum().item()
        if num_reinit > 0:
            with torch.no_grad():
                for i in range(self.num_codebooks):
                    low_usage_indices = low_usage_mask[i].nonzero(as_tuple=False).squeeze()
                    if low_usage_indices.numel() > 0:
                        self.keys.data[i, low_usage_indices, :] = torch.randn_like(self.keys.data[i, low_usage_indices, :])
                        self.values.data[i, low_usage_indices, :] = torch.randn_like(self.values.data[i, low_usage_indices, :])
                        self.key_usage_counts[i, low_usage_indices] = 0

class FullModel(nn.Module):
    def __init__(self, backbone, bottleneck, num_classes):
        super(FullModel, self).__init__()
        self.backbone = backbone
        self.bottleneck = bottleneck
        self.classifier = nn.Linear(bottleneck.num_codebooks * bottleneck.value_dim, num_classes)

    def forward(self, x):
        with torch.no_grad():
            features = self.backbone_forward(x)
        bottleneck_output = self.bottleneck(features)
        logits = self.classifier(bottleneck_output)
        return logits

    def backbone_forward(self, x):
        # Handle different backbones
        if isinstance(self.backbone, nn.Sequential):
            x = self.backbone(x)
            x = torch.flatten(x, 1)
        else:
            x = self.backbone(x.type(self.backbone.conv1.weight.dtype))
            x = x.view(x.size(0), -1)
        return x

def get_cifar10_data_loaders(phases, batch_size=256):
    transform = transforms.Compose([
        transforms.Resize(224),  # Adjust as per backbone input size
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # ImageNet normalization
    ])
    full_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

    data_loaders = []
    for phase_classes in phases:
        indices = [i for i, (_, label) in enumerate(full_dataset) if label in phase_classes]
        subset = Subset(full_dataset, indices)
        loader = DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)
        data_loaders.append(loader)
    return data_loaders

def get_cifar100_loader(batch_size=256):
    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),
    ])
    cifar100_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
    loader = DataLoader(cifar100_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    return loader

def initialize_keys(bottleneck, backbone_model, init_loader, num_epochs=10):
    bottleneck.train()
    print("Initializing keys using EMA on CIFAR-100...")
    for epoch in range(num_epochs):
        print(f"Key Initialization Epoch {epoch + 1}/{num_epochs}")
        for inputs, _ in tqdm(init_loader):
            inputs = inputs.to(device)
            with torch.no_grad():
                features = backbone_forward(backbone_model, inputs)
            bottleneck(features)
    bottleneck.eval()
    print("Key Initialization Completed.")

def backbone_forward(backbone, x):
    if isinstance(backbone, nn.Sequential):
        x = backbone(x)
        x = torch.flatten(x, 1)
    else:
        # For CLIP models or other types
        x = backbone(x.type(backbone.conv1.weight.dtype))
        if x.dim() > 2:
            x = x.view(x.size(0), -1)
    return x

def initialize_keys(bottleneck, backbone_model, init_loader, num_epochs):
    bottleneck.train()
    backbone_forward = lambda model, x: model(x).flatten(1)  # Flattens features for Key-Value Bottleneck
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Get the device
    bottleneck.to(device)  # Move the bottleneck to the device
    for epoch in range(num_epochs):
        for inputs, _ in init_loader:
            inputs = inputs.to(device) # Move inputs to the device
            with torch.no_grad():
                features = backbone_forward(backbone_model, inputs)
            bottleneck(features)
    bottleneck.eval()
    print("Key Initialization Completed.")
def backbone_forward(backbone, x):
    if isinstance(backbone, nn.Sequential):
        x = backbone(x)
        x = torch.flatten(x, 1)
    else:
        # For CLIP models or other types
        x = backbone(x.type(backbone.conv1.weight.dtype))
        if x.dim() > 2:
            x = x.view(x.size(0), -1)
    return x

def train_model(model, data_loaders, num_epochs_per_phase, learning_rates, label_smoothing=0.1):
    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    optimizer = optim.SGD([
        {'params': model.bottleneck.parameters(), 'lr': learning_rates['bottleneck']},
        {'params': model.classifier.parameters(), 'lr': learning_rates['classifier']}
    ], momentum=0.0, weight_decay=0.0)
    model.to(device)
    best_accuracy = 0.0
    for phase_idx, loader in enumerate(data_loaders):
        print(f"\nTraining Phase {phase_idx + 1}/{len(data_loaders)} with classes {loader.dataset.dataset.classes}")
        for epoch in range(num_epochs_per_phase):
            model.train()
            running_loss = 0.0
            for inputs, labels in loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item() * inputs.size(0)

            if (epoch + 1) % 100 == 0:
                epoch_loss = running_loss / len(loader.dataset)
                print(f"Phase {phase_idx + 1}, Epoch {epoch + 1}/{num_epochs_per_phase}, Loss: {epoch_loss:.4f}")
                # Key expiration
                threshold = 0.1 * len(loader.dataset) / model.bottleneck.codebook_size
                model.bottleneck.reinitialize_keys(threshold)

        # Evaluate after each phase
        accuracy = evaluate_model(model)
        print(f"Accuracy after Phase {phase_idx + 1}: {accuracy:.2f}%")
        if accuracy > best_accuracy:
            best_accuracy = accuracy
    print(f"\nBest Accuracy: {best_accuracy:.2f}%")
    return model

def evaluate_model(model):
    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    ])
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    return accuracy

def main():
    # Step 1: Choose the backbone model
    backbone_name = 'resnet50_imagenet'  # Options: 'resnet50_imagenet', 'clip_vit_b32', 'swav_resnet50w2', 'dino_resnet50'

    if backbone_name == 'resnet50_imagenet':
        backbone_model, feature_dim = get_resnet50_imagenet()
    elif backbone_name == 'clip_vit_b32':
        backbone_model, feature_dim = get_clip_vit_b32()
    elif backbone_name == 'swav_resnet50w2':
        backbone_model, feature_dim = get_swav_resnet50w2()
    elif backbone_name == 'dino_resnet50':
        backbone_model, feature_dim = get_dino_resnet50()
    else:
        raise ValueError("Invalid backbone name.")

    # Step 2: Set up Key-Value Bottleneck
    num_codebooks = 256
    codebook_size = 4096
    key_dim = feature_dim  # Match the embedding dimension from the backbone
    value_dim = 10  # Number of classes in CIFAR-10
    bottleneck = KeyValueBottleneck(num_codebooks, codebook_size, key_dim, value_dim, ema_decay=0.95)

    # Step 3: Key Initialization using CIFAR-100
    init_loader = get_cifar100_loader()
    initialize_keys(bottleneck, backbone_model, init_loader, num_epochs=10)

    # Step 4: Prepare data loaders for class-incremental learning
    phases = [[0,1], [2,3], [4,5], [6,7], [8,9]]  # Split CIFAR-10 into 5 phases with 2 classes each
    data_loaders = get_cifar10_data_loaders(phases)

    # Step 5: Initialize the full model
    model = FullModel(backbone_model, bottleneck, num_classes=10)

    # Step 6: Set up training parameters
    learning_rates = {'bottleneck': 0.3, 'classifier': 0.001}
    num_epochs_per_phase = 2000  # Training each phase for 2000 epochs may be time-consuming; adjust as needed

    # Step 7: Train the model
    trained_model = train_model(model, data_loaders, num_epochs_per_phase, learning_rates)

    # Step 8: Evaluate final performance
    final_accuracy = evaluate_model(trained_model)
    print(f"\nFinal Accuracy on CIFAR-10 test set: {final_accuracy:.2f}%")

print("sa")

if __name__ == '__main__':
    main()

#####Start from now

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import torchvision.models as models
import timm  # For SwAV and DINO models
import clip  # For CLIP model
import numpy as np
import random
from tqdm import tqdm
import os

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Set random seed for reproducibility
def set_seed(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

def get_resnet50_imagenet():
    model = models.resnet50(pretrained=True)
    feature_dim = model.fc.in_features
    # Remove the final classification layer
    model = nn.Sequential(*list(model.children())[:-1])
    model.eval()
    return model.to(device), feature_dim

def get_clip_vit_b32():
    model, _ = clip.load("ViT-B/32", device=device)
    feature_dim = model.visual.output_dim
    model = model.visual  # Use the visual part only
    model.eval()
    return model.to(device), feature_dim

def get_swav_resnet50w2():
    model = timm.create_model('swav_resnet50w2', pretrained=True)
    feature_dim = model.num_features
    # Remove the final classification layer
    model = nn.Sequential(*list(model.children())[:-1])
    model.eval()
    return model.to(device), feature_dim

def get_dino_resnet50():
    model = timm.create_model('dino_resnet50', pretrained=True)
    feature_dim = model.num_features
    # Remove the final classification layer
    model = nn.Sequential(*list(model.children())[:-1])
    model.eval()
    return model.to(device), feature_dim

class KeyValueBottleneck(nn.Module):
    def __init__(self, num_codebooks, codebook_size, key_dim, value_dim, ema_decay=0.95):
        super(KeyValueBottleneck, self).__init__()
        self.num_codebooks = num_codebooks
        self.codebook_size = codebook_size
        self.key_dim = key_dim
        self.value_dim = value_dim
        self.ema_decay = ema_decay

        # Initialize keys and values
        self.keys = nn.Parameter(torch.randn(num_codebooks, codebook_size, key_dim))
        self.values = nn.Parameter(torch.randn(num_codebooks, codebook_size, value_dim))

        # Key usage counts for thresholding
        self.register_buffer('key_usage_counts', torch.zeros(num_codebooks, codebook_size))

        # Flags
        self.initialized = False

    def forward(self, x):
        # Ensure x is on the same device as self.keys
        x = x.to(self.keys.device)

        batch_size = x.size(0)
        num_codebooks = self.num_codebooks
        codebook_size = self.codebook_size

        selected_values = []

        for i in range(num_codebooks):
            keys_i = self.keys[i]  # (codebook_size, key_dim)
            values_i = self.values[i]  # (codebook_size, value_dim)

            # Compute distances between x and keys_i
            distances = torch.cdist(x, keys_i, p=2)  # (batch_size, codebook_size)

            # Find nearest key
            indices_i = distances.argmin(dim=1)  # (batch_size)

            # Gather values
            selected_values_i = values_i[indices_i]  # (batch_size, value_dim)
            selected_values.append(selected_values_i)

            if self.training:
                if not self.initialized:
                    # Initialize keys with first batch
                    with torch.no_grad():
                        self.keys.data[i][indices_i] = x
                else:
                    # EMA update
                    keys_selected = self.keys[i][indices_i]  # (batch_size, key_dim)
                    new_keys = self.ema_decay * keys_selected + (1 - self.ema_decay) * x

                    # Handle duplicate indices using index_add_
                    delta_keys = new_keys - keys_selected  # (batch_size, key_dim)
                    keys_update = torch.zeros_like(self.keys[i]).to(self.keys.device)  # Ensure tensor is on correct device
                    keys_update.index_add_(0, indices_i, delta_keys)

                    # Update keys
                    self.keys.data[i] += keys_update

                    # Update key usage counts
                    usage = torch.bincount(indices_i, minlength=codebook_size).float().to(self.keys.device)
                    self.key_usage_counts[i] = self.ema_decay * self.key_usage_counts[i] + (1 - self.ema_decay) * usage

        selected_values = torch.cat(selected_values, dim=1)  # (batch_size, num_codebooks * value_dim)

        if self.training and not self.initialized:
            self.initialized = True

        return selected_values

    def reinitialize_keys(self, threshold):
        # Re-initialize keys with usage below the threshold
        low_usage_mask = self.key_usage_counts < threshold
        num_reinit = low_usage_mask.sum().item()
        if num_reinit > 0:
            with torch.no_grad():
                for i in range(self.num_codebooks):
                    low_usage_indices = low_usage_mask[i].nonzero(as_tuple=False).squeeze()
                    if low_usage_indices.numel() > 0:
                        self.keys.data[i][low_usage_indices] = torch.randn_like(self.keys.data[i][low_usage_indices])
                        self.values.data[i][low_usage_indices] = torch.randn_like(self.values.data[i][low_usage_indices])
                        self.key_usage_counts[i][low_usage_indices] = 0

class FullModel(nn.Module):
    def __init__(self, backbone, bottleneck, num_classes):
        super(FullModel, self).__init__()
        self.backbone = backbone
        self.bottleneck = bottleneck
        self.classifier = nn.Linear(bottleneck.num_codebooks * bottleneck.value_dim, num_classes)

    def forward(self, x):
        with torch.no_grad():
            features = self.backbone_forward(x)
        bottleneck_output = self.bottleneck(features)
        logits = self.classifier(bottleneck_output)
        return logits

    def backbone_forward(self, x):
        # Handle different backbones
        if isinstance(self.backbone, nn.Sequential):
            x = self.backbone(x)
            x = torch.flatten(x, 1)
        else:
            # For CLIP and other models
            x = self.backbone(x.type(self.backbone.conv1.weight.dtype))
            if x.dim() > 2:
                x = x.view(x.size(0), -1)
        return x

def get_cifar10_data_loaders(phases, batch_size=256):
    transform = transforms.Compose([
        transforms.Resize(224),  # Adjust as per backbone input size
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # ImageNet normalization
    ])
    full_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

    data_loaders = []
    for phase_classes in phases:
        indices = [i for i, (_, label) in enumerate(full_dataset) if label in phase_classes]
        subset = Subset(full_dataset, indices)
        loader = DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)
        data_loaders.append(loader)
    return data_loaders

def get_cifar100_loader(batch_size=256):
    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),  # CIFAR-100 normalization
    ])
    cifar100_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
    loader = DataLoader(cifar100_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    return loader

def initialize_keys(bottleneck, backbone_model, init_loader, num_epochs=10):
    bottleneck.train()
    print("Initializing keys using EMA on CIFAR-100...")
    for epoch in range(num_epochs):
        print(f"Key Initialization Epoch {epoch + 1}/{num_epochs}")
        for inputs, _ in tqdm(init_loader):
            inputs = inputs.to(device)
            with torch.no_grad():
                features = backbone_model(inputs)
                if isinstance(features, list):
                    features = features[0]
                features = features.view(features.size(0), -1)
            # Ensure features are on the same device
            features = features.to(device)
            bottleneck(features)
    bottleneck.eval()
    print("Key Initialization Completed.")

def train_model(model, data_loaders, num_epochs_per_phase, learning_rates, label_smoothing=0.1):
    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    optimizer = optim.SGD([
        {'params': model.bottleneck.parameters(), 'lr': learning_rates['bottleneck']},
        {'params': model.classifier.parameters(), 'lr': learning_rates['classifier']}
    ], momentum=0.0, weight_decay=0.0)
    model.to(device)
    best_accuracy = 0.0
    for phase_idx, loader in enumerate(data_loaders):
        print(f"\nTraining Phase {phase_idx + 1}/{len(data_loaders)}")
        for epoch in range(num_epochs_per_phase):
            model.train()
            running_loss = 0.0
            for inputs, labels in loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item() * inputs.size(0)

            if (epoch + 1) % 100 == 0 or (epoch + 1) == num_epochs_per_phase:
                epoch_loss = running_loss / len(loader.dataset)
                print(f"Phase {phase_idx + 1}, Epoch {epoch + 1}/{num_epochs_per_phase}, Loss: {epoch_loss:.4f}")
                # Key expiration
                threshold = 0.1 * len(loader.dataset) / model.bottleneck.codebook_size
                model.bottleneck.reinitialize_keys(threshold)

        # Evaluate after each phase
        accuracy = evaluate_model(model)
        print(f"Accuracy after Phase {phase_idx + 1}: {accuracy:.2f}%")
        if accuracy > best_accuracy:
            best_accuracy = accuracy
    print(f"\nBest Accuracy: {best_accuracy:.2f}%")
    return model

def evaluate_model(model):
    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    ])
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    return accuracy

def main():
    # Step 1: Choose the backbone model
    backbone_name = 'resnet50_imagenet'  # Options: 'resnet50_imagenet', 'clip_vit_b32', 'swav_resnet50w2', 'dino_resnet50'

    if backbone_name == 'resnet50_imagenet':
        backbone_model, feature_dim = get_resnet50_imagenet()
    elif backbone_name == 'clip_vit_b32':
        backbone_model, feature_dim = get_clip_vit_b32()
    elif backbone_name == 'swav_resnet50w2':
        backbone_model, feature_dim = get_swav_resnet50w2()
    elif backbone_name == 'dino_resnet50':
        backbone_model, feature_dim = get_dino_resnet50()
    else:
        raise ValueError("Invalid backbone name.")

    # Step 2: Set up Key-Value Bottleneck
    num_codebooks = 256
    codebook_size = 4096
    key_dim = feature_dim  # Match the embedding dimension from the backbone
    value_dim = 10  # Number of classes in CIFAR-10
    bottleneck = KeyValueBottleneck(num_codebooks, codebook_size, key_dim, value_dim, ema_decay=0.95)
    # Move bottleneck to the device
    bottleneck.to(device)

    # Step 3: Key Initialization using CIFAR-100
    init_loader = get_cifar100_loader()
    initialize_keys(bottleneck, backbone_model, init_loader, num_epochs=10)

    # Step 4: Prepare data loaders for class-incremental learning
    phases = [[0,1], [2,3], [4,5], [6,7], [8,9]]  # Split CIFAR-10 into 5 phases with 2 classes each
    data_loaders = get_cifar10_data_loaders(phases)

    # Step 5: Initialize the full model
    model = FullModel(backbone_model, bottleneck, num_classes=10)

    # Step 6: Set up training parameters
    learning_rates = {'bottleneck': 0.3, 'classifier': 0.001}
    num_epochs_per_phase = 200  # Adjusted for Colab

    # Step 7: Train the model
    trained_model = train_model(model, data_loaders, num_epochs_per_phase, learning_rates)

    # Step 8: Evaluate final performance
    final_accuracy = evaluate_model(trained_model)
    print(f"\nFinal Accuracy on CIFAR-10 test set: {final_accuracy:.2f}%")



def train_linear_probe(backbone_model, num_classes=10):
    # Freeze backbone
    backbone_model.eval()
    # Define a linear classifier
    classifier = nn.Linear(backbone_model[-1][-1].in_features, num_classes)
    classifier.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.0, weight_decay=0.0)

    # Prepare CIFAR-10 data loader
    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    ])
    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    loader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)

    # Training loop
    for epoch in range(10):
        classifier.train()
        running_loss = 0.0
        for inputs, labels in tqdm(loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            with torch.no_grad():
                features = backbone_model(inputs)
                features = features.view(features.size(0), -1)
            optimizer.zero_grad()
            outputs = classifier(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_loss = running_loss / len(loader.dataset)
        print(f"Linear Probe Epoch {epoch + 1}/10, Loss: {epoch_loss:.4f}")

    # Evaluate
    classifier.eval()
    correct = 0
    total = 0
    test_loader = DataLoader(datasets.CIFAR10(root='./data', train=False, download=True, transform=transform), batch_size=256, shuffle=False, num_workers=2)
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            features = backbone_model(inputs)
            features = features.view(features.size(0), -1)
            outputs = classifier(features)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print(f"Linear Probe Accuracy: {accuracy:.2f}%")

def train_one_layer_mlp(backbone_model, num_classes=10):
    # Freeze backbone
    backbone_model.eval()
    # Define a 1-layer MLP
    mlp = nn.Sequential(
        nn.Linear(backbone_model[-1][-1].in_features, 512),
        nn.ReLU(),
        nn.Linear(512, num_classes)
    )
    mlp.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(mlp.parameters(), lr=0.001, momentum=0.0, weight_decay=0.0)

    # Prepare CIFAR-10 data loader
    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    ])
    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    loader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)

    # Training loop
    for epoch in range(10):
        mlp.train()
        running_loss = 0.0
        for inputs, labels in tqdm(loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            with torch.no_grad():
                features = backbone_model(inputs)
                features = features.view(features.size(0), -1)
            optimizer.zero_grad()
            outputs = mlp(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_loss = running_loss / len(loader.dataset)
        print(f"1-layer MLP Epoch {epoch + 1}/10, Loss: {epoch_loss:.4f}")

    # Evaluate
    mlp.eval()
    correct = 0
    total = 0
    test_loader = DataLoader(datasets.CIFAR10(root='./data', train=False, download=True, transform=transform), batch_size=256, shuffle=False, num_workers=2)
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            features = backbone_model(inputs)
            features = features.view(features.size(0), -1)
            outputs = mlp(features)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print(f"1-layer MLP Accuracy: {accuracy:.2f}%")

if __name__ == '__main__':
    main()

    # After training the main model, we can train the Linear Probe and 1-Layer MLP for comparison
    print("\nTraining Linear Probe...")
    if backbone_name == 'resnet50_imagenet':
        backbone_model, _ = get_resnet50_imagenet()  # Re-initialize to ensure it's unfrozen
    train_linear_probe(backbone_model)

    print("\nTraining 1-Layer MLP...")
    if backbone_name == 'resnet50_imagenet':
        backbone_model, _ = get_resnet50_imagenet()  # Re-initialize to ensure it's unfrozen
    train_one_layer_mlp(backbone_model)